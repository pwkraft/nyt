\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{lscape}
\usepackage[colorlinks=true,citecolor=black,urlcolor=blue]{hyperref}
\setlength{\headheight}{15pt}

\title{Preliminary Analyses of New York Times Articles\footnote{The code is available on GitHub: \url{https://github.com/pwkraft/nyt}}\\\large{Version 2}}
\author{Patrick Kraft}

\begin{document}

\maketitle

<<echo=FALSE,results='hide',message=FALSE>>=

rm(list=ls())
library(stm)
library(dplyr)
library(tidyr)
library(car)
library(ggplot2)

## load data
load("../in/nyt_combined.Rdata")
load("../in/nyt_reduced.Rdata")
load("../in/nyt_polecon.Rdata")
load("../in/nyt_readab.Rdata")
load("../in/stm_select.Rdata")
load("../in/stm_polecon.Rdata")

@ 


\section{Description of Dataset}

In order to analyze the content of the NYT articles using the structural topic model approach presented by \citet{roberts2014structural}, I transformed the scraped articles to a reduced dataset where each unique article is included as a \textit{single} observation. Articles that appeared several times in the original raw collection (e.g. most tweeted article for several days) or through different channels (e.g. most tweeted and most viewed) were combined in a single observation. Overall, the reduced dataset contains \Sexpr{nrow(nyt_reduced)} \textit{unique} articles for subsequent analyses. Note that the number of articles is slightly lower than in the previous version (5592). I had to scrape the articles again in order to be able to calculate the readability indices (the previous scraping directly pre-processed the articles to omit punctuation etc.) and 88 articles could not be retrieved again in the second scraping. The missing articles were reuters/aponline press releases that were not available on the NYT website anymore.

For each observation, I created a vector of dichotomous variables indicating whether the respective article was included in each of the categories (emailed, facebook, etc.) at least once. Here is a sample of observations from this reduced dataset (article body, keywords, etc. are omitted). These variables represent the matrix of covariates that will be used in order to model differences in topical prevalence in the collection of documents.

<<echo=FALSE>>=

head(data.frame(nyt_reduced[nchar(nyt_reduced$title)<20,c(3,7:14)]))

@ 


\section{Initial Selection Model with 5 Topics}

We decided to focus our analysis on a subset of articles that contain political content. In order to select these articles, I estimated a first structural topic model with 5 topics using the \texttt{stm} package in \texttt{R} \citep[using spectral initialization, see][]{roberts2014structural,roberts2014stm}. In order to make the estimation computationally more tractable, I removed terms from the dictionary that only appeared in 10 articles or less. The following output presents an overview of the extracted topics by displaying words that are highly associated with the respective topic (using highest probability, FREX, Lift, Score, c.f. \citealt{roberts2014stm} for more details).

<<echo=FALSE>>=
labelTopics(stm_select)
@ 

The first topic clearly covers political issues and the US Presidential race. Topic 3 is mostly focused on conflicts, and topic 5 covers a mixture of economic, technology, and sports. Topics 2 and 4, on the other hand, cover cultural themes (movies, museum, fashion, etc.). The heterogeneity in topic 5 indicates that a total number of five topics is to small to properly characterize the corpus. Nevertheless, this broad categorization is sufficient to select a subset of articles related to political issues for the subsequent analyses. I omitted all articles that had the highest probability to belong to topic 2 and 4. Topic 5 was not omitted since it contains articles related to economic issues, which may well be politically relevant. As such, the filtering of political articles can be seen as conservative in the sense that we are more likely to include articles that are not clearly political rather than omitting articles that are. The reduced dataset consists of \Sexpr{nrow(nyt_polecon)} articles that were estimated to be most likely to belong to topics 1, 3, or 5.\footnote{It would also be possible to estimate a larger topic model using the entire set of articles and then only focus on topics that are clearly political. The substantive conclusions should not differ with either approach.}


\section{Results for Political/Economic Articles - 10 Topics}

After selecting the subset of articles that focus on political or economic issues, I estimated a second model with 10 topics. The following output again displays the topics along with highly associated words.

<<echo=FALSE>>=

labelTopics(stm_polecon)

@ 

The following plot displays the proportions of each individual topic in the overall text body. I added descriptive labels for each topic.

<<echo=FALSE,fig.width=4, fig.height=4>>=
topics <- c("Presidential Race","Technology","International","Supreme Court/Legal","Police","Religion (unclear)","Iran/Israel","Health/Care","Sport","Economy")
plot.STM(stm_polecon, type = "summary", custom.labels = topics, text.cex=.6)

@ 

Interestingly, the prevalence of topics in the corpus is quite evenly distributed. However, it should be kept in mind, that each observation in the document matrix can represent multiple instances of an article in the raw collection. The proportions presented here only describe the proportions in unique articles but does not take into account how often each of the articles was included originally (e.g. as most tweeted, or most viewed multiple times). 

We can also directly compare how certain words differentiate between topics. Consider for example topic 1 (presidential race) and topic 3 (international). The following plot displays how certain words are related to each of these topics. The size of the words is proportional to the frequency of occurence in the text, and the position on the x-axis describes whether the word is more related to either of the topics.

<<echo=FALSE,fig.width=6, fig.height=6>>=
plot.STM(stm_polecon, type = "perspective", topics = c(1,3))
@ 

In this example, we can see that ``Clinton'' is clearly associated with topic 1 (presidential race), rather than topic 3 (international). The term ``president'' on the other hand, is mentioned frequently in both topics, but cannot be uniquey ascribed to either of the topics. This plausible finding provides some additional face validity for the topic model.


\section{Differences in Topic Proportions between Categories}

As described in \citet{roberts2014structural}, the structural topic model not only extracts topics from a collection of documents but also allows us to directly model the prevalence of topics in specific documents based on a matrix of meta-covariates. While it is also possible to use covariates in order to model differences in words used to describe certain topics, we only focus on differences with regard to \textit{how much} a topic is discussed in specific articles. The following figures display the change in the expected proportion to discuss individual topics for articles that were included in each of the categories or not.

As a first step, we examine topic distributions for content offered by the New York Times through different channels (printed front page vs. digital sections). Looking at the articles that appearead on the printed front page, we can see that the topics ``Technology'' and ``Health/Care'' are less likely to appear. The proportion of articles focusing on both topics is about 5 percentage points lower than in articles that do not appear on the printed front page. Other topics such as ``International'' or ``Police'' are sligthly more likely to be discussed in front page articles. This pattern is amplified when looking at the top news section of the digital edition. Again, ``Technology'' and ``Health/Care'' are less likely to appear (along with ``Sport''). On the other hand, articles in this category were more likely to discuss the presidential race, international affairs, police, as well as relations with Iran and Israel. The bottom section of the digital edition basically reverses the topic distribution in the top news section. Here, the proportion of articles related to technology, health, and sport is higher. Lastly, articles in the opinion section were more likely to discuss the Supreme Court and Iran/Israel.


<<echo=FALSE,fig.width=9, fig.height=6>>=
### topic proportions in each category
prep <- estimateEffect(1:10 ~ emailed + facebook + front + tweeted + viewed +
                           digital_opinion + digital_topnews + digital_bottom
                     , stm_polecon, meta = out_polecon$meta, uncertainty = "Global")
par(mfrow = c(2,2))
plot.estimateEffect(prep, covariate = "front", topics = 1:10, model = stm_polecon
                  , xlim = c(-.3,.3), method = "difference", cov.value1 = 1, cov.value2 = 0
                  , main = "front", labeltype = "custom", custom.labels = topics)
plot.estimateEffect(prep, covariate = "digital_topnews", topics = 1:10, model = stm_polecon
                  , xlim = c(-.3,.3), method = "difference", cov.value1 = 1, cov.value2 = 0
                  , main = "digital_topnews", labeltype = "custom", custom.labels = topics)
plot.estimateEffect(prep, covariate = "digital_bottom", topics = 1:10, model = stm_polecon
                  , xlim = c(-.3,.3), method = "difference", cov.value1 = 1, cov.value2 = 0
                  , main = "digital_bottom", labeltype = "custom", custom.labels = topics)
plot.estimateEffect(prep, covariate = "digital_opinion", topics = 1:10, model = stm_polecon
                  , xlim = c(-.3,.3), method = "difference", cov.value1 = 1, cov.value2 = 0
                  , main = "digital_opinion", labeltype = "custom", custom.labels = topics)
@ 


Next, we compare topic distributions among articles that were most shared on different platforms. The figures reveal several interesting patterns. For example, we can see that articles that were most emailed were more likely to belong to the ``Health/Care'' topic but less likely to belong to political topics such as ``Presidential Race'', ``International'', or ``Supreme Court/Legal''. On the other hand, articles that were top shared on Facebook were more likely to belong to the topic ``Supreme Court/Legal''. Many articles in this topic discussed same-sex marriage and related Supreme Court decisions. As such, individuals who shared news content on Facebook were more likely to share articles related to marriage equality. Articles shared most on twitter, on the other hand, were more likely to discuss topics of ``Technology''. Overall, the results regarding shared content seem plausible in the context of underlying demographics of the population using each medium.

<<echo=FALSE,fig.width=9, fig.height=6>>=
par(mfrow = c(2,2))
plot.estimateEffect(prep, covariate = "emailed", topics = 1:10, model = stm_polecon
                  , xlim = c(-.3,.3), method = "difference", cov.value1 = 1, cov.value2 = 0
                  , main = "emailed", labeltype = "custom", custom.labels = topics)
plot.estimateEffect(prep, covariate = "facebook", topics = 1:10, model = stm_polecon
                  , xlim = c(-.3,.3), method = "difference", cov.value1 = 1, cov.value2 = 0
                  , main = "facebook", labeltype = "custom", custom.labels = topics)
plot.estimateEffect(prep, covariate = "tweeted", topics = 1:10, model = stm_polecon
                  , xlim = c(-.3,.3), method = "difference", cov.value1 = 1, cov.value2 = 0
                  , main = "tweeted", labeltype = "custom", custom.labels = topics)
plot.estimateEffect(prep, covariate = "viewed", topics = 1:10, model = stm_polecon
                  , xlim = c(-.3,.3), method = "difference", cov.value1 = 1, cov.value2 = 0
                  , main = "viewed", labeltype = "custom", custom.labels = topics)
@ 

How do these patterns translate into views? The last plot (bottom right) of the previous figure displays change in topic proportions for articles that were most viewed. Articles in this category were more likely to discuss the presidential race, even though articles in this topic were not more likely to be shared on any platform. Other topics that were more likely in the most-viewed category are ``Police'' and ``Iran/Israel''. Interestingly, while articles about health were more likely to be emailed, they were less likely to be most viewed articles.


\section{Topic Proportions over Time}

We can also examine how the proportion of topics in each of the categories changes over time. Based on the topic model, I matched the estimated topic proportions for each article with the articles' dates of appearance. The following figure displays the aggregate topic proportions for a selection of political topics in each of the article categories from mid February till end of April. Especially the peaks in topic proportions are interesting here. For example, we see that the proportion of articles related to the topic ``Supreme Court/Legal'' in the top news category has a marked increase towards the end of the covered time period. Looking at the articles that were most viewed, we can identify several time points were the presidential race as well as the topic ``Police'' was more salient than other topics.


<<echo=FALSE, message=FALSE>>=

### look at topic proportions for each category over time
topic_select <- apply(stm_select$theta, 1, function(x) which(x == max(x)))
topic_select <- topic_select == 1 | topic_select == 3 | topic_select == 5
topic_select <- nyt_reduced$title[topic_select == T]

nyt_series <- nyt_combined %>% filter(title %in% topic_select) %>% select(date, title, type) %>%
    left_join(bind_cols(select(nyt_polecon, title), data.frame(stm_polecon$theta))) %>%
    group_by(date, type) %>% select(-title) %>% summarize_each(funs(mean)) %>%
    gather("topic","proportion",3:12)
nyt_series$topic <- factor(nyt_series$topic, labels = topics)
nyt_series <- nyt_series %>% filter(topic == "Presidential Race" | topic == "International" | 
                                    topic == "Supreme Court/Legal" | topic == "Police" | 
                                    topic == "Iran/Israel")

ggplot(nyt_series, aes(x = date, y = proportion, col = topic)) + geom_line() + facet_wrap(~type) +
    theme_bw() + theme(legend.position = "bottom")

@ 

\section{Article Readability}

Another interesting question is whether there are structural differences between articles that determine whether they are more likely to be shared/most viewed etc. In order to examine whether the article complexity has any effect, I calculated the Automated Readability Index (ARI) for all articles (there are other readbility indices available, as well). The following figure displays the mean ARI scores for each article category. The ARI is supposed to map onto US grade years, so higher numbers indicate higher levels of difficulty. We can observe that articles that were most emailed are on average less complex/difficult to read than articles in any of the remaining categories.

<<echo=FALSE, message=FALSE, fig.width=5, fig.height=4>>=

### complexity by category
ci <- function(x){
    mu <- mean(x, na.rm = T)
    se <- sd(x, na.rm = T)/sqrt(length(na.omit(x)))
    ci_lo <- mu - 1.96 * se
    ci_hi <- mu + 1.96 * se
    out <- c(mu, ci_lo, ci_hi)
    return(out)
}

test <- nyt_reduced %>% mutate(readab = nyt_readab$ARI) %>% right_join(nyt_polecon) %>% data.frame()
readab_summary <- data.frame(NULL)
for(i in c("emailed", "facebook", "front", "tweeted", "viewed", "digital_opinion"
         , "digital_topnews", "digital_bottom")){
    tmp <- data.frame(t(ci(test[which(test[i] == 1), "readab"])))
    tmp$variable = i
    readab_summary <- rbind(readab_summary, tmp)
}
colnames(readab_summary)[1:3] <- c("mean","cilo","cihi")

ggplot(readab_summary, aes(y = mean, ymin = cilo, ymax = cihi, x = variable)) + geom_pointrange() + theme_bw() + coord_flip()

@ 


\section{What stays in the loop}

As a last step, I started examining the order in which articles appear in each of the categories discussed so far. For example, it could be interesting to see whether articles are first shared and then become most viewed (or vice versa), or whether articles are potentially moved to the top news section after being shared a lot etc. I am not sure yet about the best way to model such patterns since the underlying data structure is relatively complex. To get a first impression, I selected all articles that were included in the dataset for mutiple days and calculated the proportion of article categories for each day since the first publication (most shared, top news etc.). The following figure displays the results.

<<echo=FALSE, message=FALSE, fig.width=5, fig.height=4>>=

### switches between categories

nyt_switch <- nyt_combined %>% filter(title %in% topic_select) %>% select(date, title, type) %>%
    filter(title %in% unique(title[duplicated(title)])) %>% arrange(title, date) %>%
    group_by(title) %>% mutate(day = as.numeric(date - min(date))) %>% unique()
tmp <- nyt_switch %>% group_by(title) %>% summarize(maxday = max(day)) %>% filter(maxday==0)
nyt_switch <- nyt_switch %>% filter(!title %in% tmp$title) %>% count(day, type)
tmp <- nyt_switch %>% group_by(day) %>% summarize(total = sum(n))
nyt_switch <- nyt_switch %>% left_join(tmp) %>% mutate(prop = n/total) %>% filter(day <=5)

ggplot(nyt_switch, aes(x = day, y = prop, col = type)) + geom_line() + 
    theme_bw() + theme(legend.position = "bottom")

@ 

On the first day of publication, most articles only appeared in the bottom section of the digital edition (about 40\%), or the top news section (about 25\%). Almost none of the articles that appear in the dataset for multiple times have been published on the printed front page first. Rather, they appear in print the day after being available online, as can be seen by the increase in the proportion of front page articles at day 1 (and the respective decrease in digital articles). The proportion of articles belonging to the most shared or most viewed categories increase throughout the first three days. While the differences are not very large, it appears that articles fall into the categories most tweeted and most viewed earlier than in the categories most emailed or most shared on facebook. Sharing on twitter and views might therefore preceed subsequent sharing on facebook and via email. However, it is worth noting again that these differences are relatively small and are so far only examined on an aggregate level. Nevertheless, these are interesting patterns that could be investigated further.


\clearpage
\section{Conclusion / Next Steps}
Overall, the structural topic model appears to recover plausible topics from the set of articles analyzed here. I am not sure if the step-wise selection is the best procedure or whether it might make more sense to estimate a common topic model for all articles and then select articles based on political topics. The substantive results should be equivalent either way.

Possible subsequent steps:
\begin{itemize}
  \item investigate influence of other characteristics of articles (e.g. length of article)
  \item alternative measures of complexity/readability
  \item more analyses of temporal development on the level of individual articles
  \item set up hazard model, explain how long a topic stays in the cycle
\end{itemize}


\bibliographystyle{/data/Copy/1-src/lit/apsr2006}
\bibliography{/data/Copy/1-src/lit/Literature}
\end{document}

